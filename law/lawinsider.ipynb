{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a0882e-28f2-4c95-99d6-c2837de0fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 10/10 [00:00<00:00, 2649.76it/s]\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtins.safe_open' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m test_dataset = test_dataset.batch(EVAL_BATCH_SIZE)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# === LOAD AND COMPILE MODEL ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m model = \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m optimizer = tf.keras.optimizers.Adam(learning_rate=\u001b[32m3e-5\u001b[39m)\n\u001b[32m    100\u001b[39m loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tf_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tf_env/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2964\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework=\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m safetensors_archive:\n\u001b[32m   2961\u001b[39m         \u001b[38;5;66;03m# Load from a PyTorch safetensors checkpoint\u001b[39;00m\n\u001b[32m   2962\u001b[39m         \u001b[38;5;66;03m# We load in TF format here because PT weights often need to be transposed, and this is much\u001b[39;00m\n\u001b[32m   2963\u001b[39m         \u001b[38;5;66;03m# faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2964\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_state_dict_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m            \u001b[49m\u001b[43msafetensors_archive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to build the model again\u001b[39;49;00m\n\u001b[32m   2968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m safetensors_from_pt:\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_sharded_pytorch_safetensors_in_tf2_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tf_env/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:333\u001b[39m, in \u001b[36mload_pytorch_state_dict_in_tf2_model\u001b[39m\u001b[34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Convert old format to new format if needed from a PyTorch state_dict\u001b[39;00m\n\u001b[32m    332\u001b[39m tf_keys_to_pt_keys = {}\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'builtins.safe_open' object is not iterable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# === CONFIGURATION CONSTANTS ===\n",
    "LANGUAGE = 'en'\n",
    "MODEL_TYPE = 'distilbert-base-uncased' if LANGUAGE == 'en' else 'distilbert-base-multilingual-cased'\n",
    "MODEL_INPUTS = frozenset(['input_ids', 'attention_mask'])\n",
    "BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = BATCH_SIZE * 2\n",
    "EPOCHS = 2       # keep low for demo\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NUM_DOCS = 10   # mock dataset size\n",
    "\n",
    "# === MOCK Avro-Like Data Generator ===\n",
    "def mock_avro_reader(num_docs=NUM_DOCS):\n",
    "    samples = [\n",
    "        (\"<p>This agreement is made between the company and the contractor.</p>\", True),\n",
    "        (\"<h1>Invoice</h1><p>Date: 2024-09-12</p>\", False),\n",
    "        (\"<div>Employment contract for the following individual...</div>\", True),\n",
    "        (\"<p>Purchase order for materials...</p>\", False),\n",
    "        (\"<div>Lease agreement details: terms, parties, rent.</div>\", True),\n",
    "        (\"<p>Meeting notes: Project timeline, deliverables, next steps...</p>\", False),\n",
    "    ]\n",
    "    for _ in range(num_docs):\n",
    "        html, is_contract = random.choice(samples)\n",
    "        yield {\n",
    "            'body': html,\n",
    "            'labels': {'contract': is_contract}\n",
    "        }\n",
    "\n",
    "# === MOCK html_labels2text ===\n",
    "def html_labels2text(doc):\n",
    "    # Very simple HTML-stripping mock\n",
    "    text = re.sub('<[^<]+?>', '', doc['body'])\n",
    "    return {'text': text}\n",
    "\n",
    "# === PREPARE LABELLED DATA ===\n",
    "labels_with_text = []\n",
    "avro_reader = mock_avro_reader(NUM_DOCS)\n",
    "for document in avro_reader:\n",
    "    document_text = html_labels2text({'body': document['body']})['text']\n",
    "    label = 1 if document['labels'].get('contract') else 0\n",
    "    labels_with_text.append((label, document_text))\n",
    "\n",
    "# === LOAD TOKENIZER ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "def encode(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf',\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return {k: v for k, v in inputs.items() if k in MODEL_INPUTS}\n",
    "\n",
    "# === ENCODE DATA ===\n",
    "data = [\n",
    "    ({k: v[0].numpy() for k, v in encode(text).items()}, label)\n",
    "    for label, text in tqdm(labels_with_text, desc=\"Encoding texts\")\n",
    "]\n",
    "\n",
    "# === TF DATASET CREATION ===\n",
    "def _data():\n",
    "    yield from data\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    _data,\n",
    "    ({k: tf.int32 for k in MODEL_INPUTS}, tf.int32),\n",
    "    (\n",
    "        {k: tf.TensorShape([MAX_SEQ_LENGTH]) for k in MODEL_INPUTS},\n",
    "        tf.TensorShape([]),\n",
    "    ),\n",
    ")\n",
    "dataset = dataset.shuffle(\n",
    "    buffer_size=NUM_DOCS, seed=0, reshuffle_each_iteration=False\n",
    ")\n",
    "\n",
    "# === SPLIT TRAIN AND TEST ===\n",
    "total_size = len(data)\n",
    "train_size = int(0.9 * total_size)\n",
    "test_size = total_size - train_size\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat(-1)\n",
    "test_dataset = test_dataset.batch(EVAL_BATCH_SIZE)\n",
    "\n",
    "# === LOAD AND COMPILE MODEL ===\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=2)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = test_size // EVAL_BATCH_SIZE\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# === EVALUATE MODEL ===\n",
    "results = model.evaluate(test_dataset, steps=validation_steps)\n",
    "print(f\"\\nTest Loss: {results[0]:.4f} | Test Accuracy: {results[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55889092-fa15-4e0c-8240-7f551ffa75af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
